{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 19:07:23 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-03-31 19:07:23 INFO: Use device: cpu\n",
      "2022-03-31 19:07:23 INFO: Loading: tokenize\n",
      "2022-03-31 19:07:23 INFO: Loading: pos\n",
      "2022-03-31 19:07:23 INFO: Loading: lemma\n",
      "2022-03-31 19:07:23 INFO: Loading: depparse\n",
      "2022-03-31 19:07:24 INFO: Loading: sentiment\n",
      "2022-03-31 19:07:24 INFO: Loading: constituency\n",
      "2022-03-31 19:07:25 INFO: Loading: ner\n",
      "2022-03-31 19:07:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b\\ he\\b|\\b\\ him\\b|\\b\\ his\\b|\\b\\ she\\b|\\b\\ her\\b|\\b\\ hers\\b\n",
      "It was another early sunset on a rainy day in LOCATION. PERSON was walking with a paper bag of groceries back to THEIR downtown studio apartment at ADDRESS. He had gotten government permission to stay in this apartment as part of the ORGANIZATION program, focusing on bettering drug-affected neighborhoods in LOCATION. It was an artist supplement program, and PERSON was a published haiku poet and teacher of haiku in public education and sometimes colleges. He had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown LOCATION for $800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of THEIR haiku and accompanying minimalist art made by THEIR neighbor, PERSON.\n"
     ]
    }
   ],
   "source": [
    "input_paragraph = \"It was another early sunset on a rainy day in Seattle. Andrew was walking with a paper bag of groceries back to his downtown studio apartment at 400 Main Street. He had gotten government permission to stay in this apartment as part of the Upbeat program, focusing on bettering drug-affected neighborhoods in King County. It was an artist supplement program, and Andrew was a published haiku poet and teacher of haiku in public education and sometimes colleges. He had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown Seattle for $800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of his haiku and accompanying minimalist art made by his neighbor, Patrick.\"\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "doc = nlp(input_paragraph)\n",
    "# Doesn't include date, cardinal, maybe more\n",
    "lst_entities = {\n",
    "    'FAC': 'ADDRESS',\n",
    "    'GPE': 'LOCATION',\n",
    "    'PERSON': 'PERSON',\n",
    "    'ORG': 'ORGANIZATION'\n",
    "}\n",
    "for sentence in doc.sentences:\n",
    "#     print(sentence.ents)\n",
    "    for i in sentence.ents:\n",
    "        if i.type in lst_entities:\n",
    "            input_paragraph = input_paragraph.replace(i.text,lst_entities[i.type])\n",
    "replacements = {' he': ' THEY',\n",
    "                      ' him': ' THEM',\n",
    "                      ' his': ' THEIR',\n",
    "                      ' she': ' THEY',\n",
    "                      ' her': ' THEM',\n",
    "                      ' hers': ' THEIR',\n",
    "                     }\n",
    "\n",
    "def replace(match):\n",
    "    return replacements[match.group(0)]\n",
    "\n",
    "# notice that the 'this' in 'thistle' is not matched \n",
    "regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in replacements)\n",
    "print(regex)\n",
    "new_txt = re.sub(regex, replace, input_paragraph)\n",
    "print(new_txt)\n",
    "\n",
    "# for key, value in lst_manual_changes.items():\n",
    "#     print(key,value)\n",
    "#     input_paragraph = input_paragraph.replace(key,value)\n",
    "# print(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 19:08:24 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-03-31 19:08:24 INFO: Use device: cpu\n",
      "2022-03-31 19:08:24 INFO: Loading: tokenize\n",
      "2022-03-31 19:08:24 INFO: Loading: pos\n",
      "2022-03-31 19:08:25 INFO: Loading: lemma\n",
      "2022-03-31 19:08:25 INFO: Loading: depparse\n",
      "2022-03-31 19:08:25 INFO: Loading: sentiment\n",
      "2022-03-31 19:08:25 INFO: Loading: constituency\n",
      "2022-03-31 19:08:26 INFO: Loading: ner\n",
      "2022-03-31 19:08:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b\\ he\\b|\\b\\ him\\b|\\b\\ his\\b|\\b\\ she\\b|\\b\\ her\\b|\\b\\ hers\\b\n",
      "His nose could have been mistaken for a carrot. Not one of those dirt-rich oblong carrots, but a baby-sized carrot packed in those free-moving, punctured plastic bags. Yet more distinguishable were THEIR cheeks, slit in as if a wild animal unintentionally scathed a straight line down the skin by THEIR cheekbone. A rugged PERSON if I ever saw one. But THEIR name wasn’t that beefy: PERSON. PERSON was a name white collar folk call in for Sunday dinner for tidbits of entertainment or a chiseled smile. PERSON seemed like a more convincing name for THEM, especially since that nose of THEIR could have poked PERSON back to eating crêpes—but they would have it that this gritty-faced man (we haven’t gotten into THEIR personality yet) was named PERSON at birth. No, it’s not what you think—it isn’t PERSON. Parents do the darndest things.\n"
     ]
    }
   ],
   "source": [
    "input_paragraph = \"His nose could have been mistaken for a carrot. Not one of those dirt-rich oblong carrots, but a baby-sized carrot packed in those free-moving, punctured plastic bags. Yet more distinguishable were his cheeks, slit in as if a wild animal unintentionally scathed a straight line down the skin by his cheekbone. A rugged Pinocchio if I ever saw one. But his name wasn’t that beefy: Marty. Marty was a name white collar folk call in for Sunday dinner for tidbits of entertainment or a chiseled smile. Jared seemed like a more convincing name for him, especially since that nose of his could have poked Napoleon back to eating crêpes—but they would have it that this gritty-faced man (we haven’t gotten into his personality yet) was named Marty at birth. No, it’s not what you think—it isn’t Martin. Parents do the darndest things.\"\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "doc = nlp(input_paragraph)\n",
    "# Doesn't include date, cardinal, maybe more\n",
    "lst_entities = {\n",
    "    'FAC': 'ADDRESS',\n",
    "    'GPE': 'LOCATION',\n",
    "    'PERSON': 'PERSON',\n",
    "    'ORG': 'ORGANIZATION'\n",
    "}\n",
    "for sentence in doc.sentences:\n",
    "#     print(sentence.ents)\n",
    "    for i in sentence.ents:\n",
    "        if i.type in lst_entities:\n",
    "            input_paragraph = input_paragraph.replace(i.text,lst_entities[i.type])\n",
    "replacements = {' he': ' THEY',\n",
    "                      ' him': ' THEM',\n",
    "                      ' his': ' THEIR',\n",
    "                      ' she': ' THEY',\n",
    "                      ' her': ' THEM',\n",
    "                      ' hers': ' THEIR',\n",
    "                     }\n",
    "\n",
    "def replace(match):\n",
    "    return replacements[match.group(0)]\n",
    "\n",
    "# notice that the 'this' in 'thistle' is not matched \n",
    "regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in replacements)\n",
    "print(regex)\n",
    "new_txt = re.sub(regex, replace, input_paragraph)\n",
    "print(new_txt)\n",
    "\n",
    "# for key, value in lst_manual_changes.items():\n",
    "#     print(key,value)\n",
    "#     input_paragraph = input_paragraph.replace(key,value)\n",
    "# print(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
