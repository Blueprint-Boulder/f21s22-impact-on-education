{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a14c41-372b-4c93-ab66-8f5338d787e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 possible ways to get text below in the 2 methods\n",
    "#Other possibly useful links:\n",
    "    #https://blog.apilayer.com/build-your-own-resume-parser-using-python-and-nlp/\n",
    "    #https://www.geeksforgeeks.org/pdf-redaction-using-python/\n",
    "\n",
    "\n",
    "\n",
    "import PyPDF2\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en.examples import sentences\n",
    "\n",
    "# Code from https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "def read_pdf(filename):\n",
    "    # creating a pdf file object\n",
    "    pdfFileObj = open(filename, 'rb')\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "    # printing number of pages in pdf file\n",
    "    print(pdfReader.numPages)\n",
    "\n",
    "    # creating a page object\n",
    "    pageObj = pdfReader.getPage(0)\n",
    "\n",
    "    # extracting text from page\n",
    "    print(pageObj.extractText())\n",
    "\n",
    "    #to save the text and run NLP instead of print\n",
    "    resume_text = pageObj.extractText()\n",
    "\n",
    "    # closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "\n",
    "#code from - https://stackoverflow.com/questions/55220455/convert-from-pdf-to-text-lines-and-words-are-broken\n",
    "def extract_with_pdf_miner():\n",
    "    import io\n",
    "    from io import StringIO\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.converter import TextConverter\n",
    "    from pdfminer.layout import LAParams\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "    import os\n",
    "    import sys, getopt\n",
    "\n",
    "    # converts pdf, returns its text content as a string\n",
    "    def convert(fname, pages=None):\n",
    "        if not pages:\n",
    "            pagenums = set()\n",
    "        else:\n",
    "            pagenums = set(pages)\n",
    "\n",
    "        output = io.StringIO()\n",
    "        manager = PDFResourceManager()\n",
    "        converter = TextConverter(manager, output, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "        infile = open(fname, 'rb')\n",
    "        for page in PDFPage.get_pages(infile, pagenums):\n",
    "            interpreter.process_page(page)\n",
    "        infile.close()\n",
    "        converter.close()\n",
    "        text = output.getvalue()\n",
    "        output.close\n",
    "        return text\n",
    "\n",
    "        # converts all pdfs in directory pdfDir, saves all resulting txt files to txtdir\n",
    "\n",
    "    def convertMultiple(pdfDir, txtDir):\n",
    "        if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\"  # if no pdfDir passed in\n",
    "        for pdf in os.listdir(pdfDir):  # iterate through pdfs in pdf directory\n",
    "            fileExtension = pdf.split(\".\")[-1]\n",
    "            if fileExtension == \"pdf\":\n",
    "                pdfFilename = pdfDir + pdf\n",
    "                text = convert(pdfFilename)  # get string of text content of pdf\n",
    "                textFilename = txtDir + pdf + \".txt\"\n",
    "                textFile = open(textFilename, \"w\")  # make text file\n",
    "                textFile.write(text)  # write text to text file\n",
    "\n",
    "    # set paths accordingly:\n",
    "    pdfDir = \"./\"\n",
    "    txtDir = \"./\"\n",
    "    convertMultiple(pdfDir, txtDir)\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fbb6aefa-4934-4fbe-90c1-41018c90b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['ma@gmail.com']\n",
      "Noun phrases: ['My email', 'ma@gmail.com', 'My phone number', 'I', '123 Main Street', 'It', 'another early sunset', 'a rainy day', 'Seattle', 'Andrew', 'a paper bag', 'groceries', 'his downtown studio apartment', '400 Main Street', 'He', 'government permission', 'this apartment', 'part', 'the Upbeat program', 'drug-affected neighborhoods', 'King County', 'It', 'an artist supplement program', 'Andrew', 'a published haiku poet', 'teacher', 'haiku', 'public education', 'sometimes colleges', 'He', 'an audition', 'artistic achievements', 'the subsidy', 'it', 'a miracle', 'downtown Seattle', 'it', 'its downsides', 'junkies', 'the weekly exhibitions', 'his haiku', 'accompanying minimalist art', 'his neighbor', 'Patrick']\n",
      "Verbs: ['live', 'walk', 'get', 'stay', 'focus', 'better', 'affect', 'publish', 'have', 'attend', 'get', 'live', 'have', 'visit', 'accompany', 'make']\n",
      "303 CARDINAL\n",
      "Seattle GPE\n",
      "400 CARDINAL\n",
      "Main Street FAC\n",
      "Upbeat ORG\n",
      "King County GPE\n",
      "Andrew PERSON\n",
      "Seattle GPE\n",
      "800 MONEY\n",
      "weekly DATE\n",
      "Patrick ORG\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import io\n",
    "from io import StringIO\n",
    "import os\n",
    "import sys, getopt\n",
    "import re\n",
    "\n",
    "extract_with_pdf_miner()\n",
    "f = open(\"./sample_cover_letter.pdf.txt\")\n",
    "text = f.read()\n",
    "\n",
    "#f = open(\"./names.txt\") # Breaks here, Jupyter crashes\n",
    "#text = f.read()\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LIKE_EMAIL\": True}] # Email Address Pattern\n",
    "matcher.add(\"email_address\", [pattern])\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"My email is ma@gmail.com. My phone number is 303-303-3033. I live at 123 Main Street. It was another early sunset on a rainy day in Seattle. Andrew was walking with a paper bag of groceries back to his downtown studio apartment at 400 Main Street. He had gotten government permission to stay in this apartment as part of the Upbeat program, focusing on bettering drug-affected neighborhoods in King County. It was an artist supplement program, and Andrew was a published haiku poet and teacher of haiku in public education and sometimes colleges. He had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown Seattle for $800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of his haiku and accompanying minimalist art made by his neighbor, Patrick.\")\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "count = 0\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    if(entity.label_ == \"PERSON\"):\n",
    "        count+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da0109cc-200d-4003-97ec-f25d75f55870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My email is [REDACTED]. My phone number is [PHONE_NUMBER]. I live at 123 [ADDRESS]. It was another early sunset on a rainy day in [LOCATION]. [PERSON] was walking with a paper bag of groceries back to THEIR downtown studio apartment at [LOCATION] [ADDRESS]. THEY had gotten government permission to stay in tTHEIR apartment as part of the Upbeat program, focusing on bettering drug-affected neighborhoods in [LOCATION]. It was an artist supplement program, and [PERSON] was a published haiku poet and teacher of haiku in public education and sometimes colleges. THEY had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown [LOCATION] for $800 a month, it did have its downsides, with junkies visiting the [DATE] exhibitions of THEIR haiku and accompanying minimalist art made by THEIR neighbor, Patrick.\n"
     ]
    }
   ],
   "source": [
    "lst_entities = {\n",
    "    'FAC': 'ADDRESS',\n",
    "    'GPE': 'LOCATION',\n",
    "    'PERSON': 'PERSON',\n",
    "    'CARDINAL': 'LOCATION',\n",
    "    'DATE': 'DATE'\n",
    "}\n",
    "entities = {}\n",
    "for entity in doc.ents:\n",
    "    entities[entity.text] = entity.label_\n",
    "new_text = text\n",
    "pronouns = {\"he\": \"THEY\",\n",
    "                      \"him\": \"THEM\",\n",
    "                      \"his\": \"THEIR\",\n",
    "                      \"she\": \"THEY\",\n",
    "                      \"her\": \"THEM\",\n",
    "                      \"hers\": \"THEIR\",\n",
    "                      \"He\": \"THEY\",\n",
    "                      \"His\": \"THEIR\",\n",
    "                      \"She\": \"THEY\",\n",
    "                      \"Her\": \"THEM\",\n",
    "                     }\n",
    "persons = [\"man\",\"Man\",\"woman\",\"Woman\",\"non-binary\",\"Non-binary\"]\n",
    "def replace(match):\n",
    "    return pronouns[match.group(0)]\n",
    "for i in range(0,len(doc)):\n",
    "    for j in range(i+1,len(doc)):\n",
    "        token = doc[i:j]\n",
    "        if(token.text in entities):\n",
    "            if(entities[token.text] in lst_entities):\n",
    "                new_text = new_text.replace(token.text,\"[\" + lst_entities[entities[token.text]] + \"]\")\n",
    "            else:\n",
    "                pass\n",
    "        elif(token.text in persons):\n",
    "            new_text = new_text.replace(token.text,\"[PERSON]\")\n",
    "        elif(token.text in pronouns):\n",
    "            new_text = new_text.replace(token.text,pronouns[token.text])\n",
    "        else:\n",
    "            for match_id, start, end in matches: # Just emails here for now\n",
    "                if(doc[start:end].text==token.text):\n",
    "                    new_text = new_text.replace(token.text,\"[REDACTED]\")\n",
    "                    \n",
    "            PHONE_NUMBER = \"\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}\"\n",
    "            DATE = \"([\\d]{1,2}[/-][\\d]{1,2}[/-][\\d]{4})|((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s?[\\d{1,2}]?[,]?\\s+\\d{4})\"\n",
    "\n",
    "            # From https://spacy.io/usage/rule-based-matching#regex-text\n",
    "            lst_regex = {\"PHONE_NUMBER\":PHONE_NUMBER,\"DATE\":DATE}\n",
    "            for regex in lst_regex:\n",
    "                for match in re.finditer(lst_regex[regex], doc.text):\n",
    "                    start, end = match.span()\n",
    "                    span = doc.char_span(start, end)\n",
    "                    if span is not None:\n",
    "                        new_text = new_text.replace(span.text,\"[\" + str(regex) + \"]\")\n",
    "            regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in pronouns)\n",
    "            new_txt = re.sub(regex, replace, new_text)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a363fd-2ff4-4fbd-a85b-e6ba029203a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
