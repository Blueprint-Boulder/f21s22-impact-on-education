{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbb6aefa-4934-4fbe-90c1-41018c90b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: []\n",
      "Noun phrases: ['My email', 'It', 'another early sunset', 'a rainy day', 'Seattle', 'Andrew', 'a paper bag', 'groceries', 'his downtown studio apartment', '400 Main Street', 'He', 'government permission', 'this apartment', 'part', 'the Upbeat program', 'drug-affected neighborhoods', 'King County', 'It', 'an artist supplement program', 'Andrew', 'a published haiku poet', 'teacher', 'haiku', 'public education', 'sometimes colleges', 'He', 'an audition', 'artistic achievements', 'the subsidy', 'it', 'a miracle', 'downtown Seattle', 'it', 'its downsides', 'junkies', 'the weekly exhibitions', 'his haiku', 'accompanying minimalist art', 'his neighbor', 'Patrick']\n",
      "Verbs: ['walk', 'get', 'stay', 'focus', 'better', 'affect', 'publish', 'have', 'attend', 'get', 'live', 'have', 'visit', 'accompany', 'make']\n",
      "Seattle GPE\n",
      "400 CARDINAL\n",
      "Main Street FAC\n",
      "Upbeat ORG\n",
      "King County GPE\n",
      "Andrew PERSON\n",
      "Seattle GPE\n",
      "800 MONEY\n",
      "weekly DATE\n",
      "Patrick ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"POS\": \"CARDINAL\"},{\"POS\": \"FAC\"}] # Trying to identify address pattern\n",
    "matcher.add(\"address\", [pattern])\n",
    "\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"My email is dummyemail@gmail.com. It was another early sunset on a rainy day in Seattle. Andrew was walking with a paper bag of groceries back to his downtown studio apartment at 400 Main Street. He had gotten government permission to stay in this apartment as part of the Upbeat program, focusing on bettering drug-affected neighborhoods in King County. It was an artist supplement program, and Andrew was a published haiku poet and teacher of haiku in public education and sometimes colleges. He had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown Seattle for $800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of his haiku and accompanying minimalist art made by his neighbor, Patrick.\")\n",
    "        \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0109cc-200d-4003-97ec-f25d75f55870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My email is [Email]. It was another early sunset on a rainy day in [Location] . [Name 0] was walking with a paper bag of groceries back to [Pronoun] downtown studio apartment at 400 [Location] [Location] . [Pronoun] had gotten government permission to stay in this apartment as part of the Upbeat program, focusing on bettering drug- affected neighborhoods in [Location] County. It was an artist supplement program, and [Name 0] was a published haiku poet and teacher of haiku in public education and sometimes colleges. [Pronoun] had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown [Location] for $ 800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of [Pronoun] haiku and accompanying minimalist art made by [Pronoun] neighbor, Patrick.\n"
     ]
    }
   ],
   "source": [
    "new_text = \"\"\n",
    "persons = []\n",
    "locations = []\n",
    "pronouns = [\"her\",\"Her\",\"him\",\"Him\",\"he\",\"He\",\"she\",\"She\",\"his\",\"His\"]\n",
    "for entity in doc.ents:\n",
    "    if (entity.label_ == \"PERSON\"): persons.append(entity.text)\n",
    "    if (entity.label_ == \"GPE\"): locations.append(entity.text)\n",
    "    if (entity.label_ == \"FAC\"): \n",
    "        x = entity.text.split()\n",
    "        for y in x:\n",
    "            locations.append(y)\n",
    "for token in doc:\n",
    "    string = str(token.text)\n",
    "    #print(string)\n",
    "    if(token.i+1 < len(doc)): \n",
    "        next_token = doc[token.i+1]\n",
    "    if(token.text in persons):\n",
    "        new_text+=\"[Name \" + str(persons.index(token.text)) + \"] \"\n",
    "    elif(token.text.find('@') > -1):\n",
    "        new_text+=\"[Email]\"\n",
    "    elif(str(token.text + \" \" + next_token.text) in persons):\n",
    "        new_text+=\"[Name \" + str(persons.index(next_token.text)) + \"] \"\n",
    "    elif(token.text in pronouns):\n",
    "        new_text+=\"[Pronoun] \"\n",
    "    elif(token.text == \"man\" or token.text == \"woman\" or token.text == \"non-binary\"):\n",
    "        new_text+=\"person \"\n",
    "    elif(token.text in locations):\n",
    "        new_text+=\"[Location] \"\n",
    "    elif(token.pos_ == \"CARDINAL\" and next_token.text in locations):\n",
    "        new_text+=\"[Location] \"\n",
    "    elif(str(token.text + \" \" + next_token.text) in locations):\n",
    "        new_text+=\"[Location] \"\n",
    "    elif((token.pos_ == \"AUX\" and next_token.text == \"n\\'t\") or (next_token.pos_ == \"PUNCT\")):\n",
    "        new_text+=token.text + \"\"\n",
    "    else:\n",
    "        new_text+=token.text + \" \"\n",
    "print(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a363fd-2ff4-4fbd-a85b-e6ba029203a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for token in doc:\n",
    "    if(token.i+1 < len(doc)): \n",
    "        next_token = doc[token.i+1]\n",
    "    if(token.text in persons):\n",
    "        new_text+=\"[Name \" + str(persons.index(token.text)) + \"] \"\n",
    "    elif(str(token.text + \" \" + next_token.text) in persons):\n",
    "        new_text+=\"[Name \" + str(persons.index(next_token.text)) + \"] \"\n",
    "    elif((token.text == \"her\" or token.text == \"him\") and next_token.pos_ == \"PUNCT\"):\n",
    "        new_text+=\"them \"\n",
    "    elif(token.text == \"she\" or token.text == \"he\"):\n",
    "        new_text+=\"they \"\n",
    "    elif(token.text == \"She\" or token.text == \"He\"):\n",
    "        new_text+=\"They \"\n",
    "    elif(token.text == \"her\" or token.text == \"his\"):\n",
    "        new_text+=\"their \"\n",
    "    elif(token.text == \"Her\" or token.text == \"His\"):\n",
    "        new_text+=\"Their \"\n",
    "    elif(token.text == \"him\"):\n",
    "        new_text+=\"them \"\n",
    "    elif(token.text == \"Him\"):\n",
    "        new_text+=\"Them \"\n",
    "    elif(token.text == \"man\" or token.text == \"woman\"):\n",
    "        new_text+=\"person \"\n",
    "    elif(token.text in locations):\n",
    "        new_text+=\"[Location] \"\n",
    "    elif(str(token.text + \" \" + next_token.text) in locations):\n",
    "        new_text+=\"[Location] \"\n",
    "    elif((token.pos_ == \"AUX\" and next_token.text == \"n\\'t\") or (next_token.pos_ == \"PUNCT\")):\n",
    "        new_text+=token.text + \"\"\n",
    "    else:\n",
    "        new_text+=token.text + \" \"\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
