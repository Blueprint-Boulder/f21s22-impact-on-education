{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb6aefa-4934-4fbe-90c1-41018c90b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode', 'He', 'his other experiences', 'No one', 'him', 'James', 'Denver', 'me', 'It', 'her']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say', 'go', 'describe', 'speak']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n",
      "James PERSON\n",
      "Denver GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"POS\": \"PRON\"},{\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn't \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week. He then went on to describe his other experiences. \"\n",
    "        \"No one would speak to him. James in Denver was nice to me. It was very sad for her.\")\n",
    "        \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0109cc-200d-4003-97ec-f25d75f55870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When [Name] [Name] started working on self- driving cars at Google in 2007, few people outside of the company took them seriously.“ I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn't worth talking to,” said [Name] , in an interview with Recode earlier this week. They then went on to describe their other experiences. No one would speak to them . [Name] in [Location] was nice to me. It was very sad for them .\n"
     ]
    }
   ],
   "source": [
    "new_text = \"\"\n",
    "persons = []\n",
    "locations = []\n",
    "for entity in doc.ents:\n",
    "    if (entity.label_ == \"PERSON\"): persons.append(entity.text)\n",
    "    if (entity.label_ == \"GPE\"): locations.append(entity.text);\n",
    "for token in doc:\n",
    "    if(token.i+1 < len(doc)): \n",
    "        next_token = doc[token.i+1]\n",
    "    if(token.text in persons or str(token.text + \" \" + next_token.text) in persons):\n",
    "        new_text+=\"[Name] \"\n",
    "    elif((token.text == \"her\" or token.text == \"him\") and next_token.pos_ == \"PUNCT\"):\n",
    "        new_text+=\"them \"\n",
    "    elif(token.text == \"she\" or token.text == \"he\"):\n",
    "        new_text+=\"they \"\n",
    "    elif(token.text == \"She\" or token.text == \"He\"):\n",
    "        new_text+=\"They \"\n",
    "    elif(token.text == \"her\" or token.text == \"his\"):\n",
    "        new_text+=\"their \"\n",
    "    elif(token.text == \"Her\" or token.text == \"His\"):\n",
    "        new_text+=\"Their \"\n",
    "    elif(token.text == \"him\"):\n",
    "        new_text+=\"them \"\n",
    "    elif(token.text == \"Him\"):\n",
    "        new_text+=\"Them \"\n",
    "    elif(token.text in locations):\n",
    "        new_text+=\"[Location] \"\n",
    "    elif((token.pos_ == \"AUX\" and next_token.text == \"n\\'t\") or (next_token.pos_ == \"PUNCT\")):\n",
    "        new_text+=token.text + \"\"\n",
    "    else:\n",
    "        new_text+=token.text + \" \"\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a363fd-2ff4-4fbd-a85b-e6ba029203a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
