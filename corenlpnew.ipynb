{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 14:01:35 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-04-12 14:01:35 INFO: Use device: cpu\n",
      "2022-04-12 14:01:35 INFO: Loading: tokenize\n",
      "2022-04-12 14:01:35 INFO: Loading: pos\n",
      "2022-04-12 14:01:35 INFO: Loading: lemma\n",
      "2022-04-12 14:01:35 INFO: Loading: depparse\n",
      "2022-04-12 14:01:36 INFO: Loading: sentiment\n",
      "2022-04-12 14:01:36 INFO: Loading: constituency\n",
      "2022-04-12 14:01:37 INFO: Loading: ner\n",
      "2022-04-12 14:01:37 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checklist:\n",
    "\n",
    "# Websites? Find and replace .com or http\n",
    "\n",
    "# Location Info -> NLP\n",
    "\n",
    "# DONE:\n",
    "# Company / Organization Names -> NLP\n",
    "# Names -> NLP \n",
    "# Emails -> Regex \n",
    "# Pronouns -> Dictionary replace\n",
    "# Dates -> Regex\n",
    "# Phone Numbers -> Regex\n",
    "# Gender -> Dictionary replace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code from https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "def read_pdf(filename):\n",
    "    # creating a pdf file object\n",
    "    pdfFileObj = open(filename, 'rb')\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "    # printing number of pages in pdf file\n",
    "    print(pdfReader.numPages)\n",
    "\n",
    "    # creating a page object\n",
    "    pageObj = pdfReader.getPage(0)\n",
    "\n",
    "    # extracting text from page\n",
    "    print(pageObj.extractText())\n",
    "\n",
    "    #to save the text and run NLP instead of print\n",
    "    resume_text = pageObj.extractText()\n",
    "\n",
    "    # closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "\n",
    "#code from - https://stackoverflow.com/questions/55220455/convert-from-pdf-to-text-lines-and-words-are-broken\n",
    "def extract_with_pdf_miner():\n",
    "    from io import StringIO\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.converter import TextConverter\n",
    "    from pdfminer.layout import LAParams\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "    import os\n",
    "    import sys, getopt\n",
    "\n",
    "    # converts pdf, returns its text content as a string\n",
    "    def convert(fname, pages=None):\n",
    "        if not pages:\n",
    "            pagenums = set()\n",
    "        else:\n",
    "            pagenums = set(pages)\n",
    "\n",
    "        output = io.StringIO()\n",
    "        manager = PDFResourceManager()\n",
    "        converter = TextConverter(manager, output, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "        infile = open(fname, 'rb')\n",
    "        for page in PDFPage.get_pages(infile, pagenums):\n",
    "            interpreter.process_page(page)\n",
    "        infile.close()\n",
    "        converter.close()\n",
    "        text = output.getvalue()\n",
    "        output.close\n",
    "        return text\n",
    "\n",
    "        # converts all pdfs in directory pdfDir, saves all resulting txt files to txtdir\n",
    "\n",
    "    def convertMultiple(pdfDir, txtDir):\n",
    "        if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\"  # if no pdfDir passed in\n",
    "        for pdf in os.listdir(pdfDir):  # iterate through pdfs in pdf directory\n",
    "            fileExtension = pdf.split(\".\")[-1]\n",
    "            if fileExtension == \"pdf\":\n",
    "                pdfFilename = pdfDir + pdf\n",
    "                text = convert(pdfFilename)  # get string of text content of pdf\n",
    "                textFilename = txtDir + pdf + \".txt\"\n",
    "                textFile = open(textFilename, \"w\")  # make text file\n",
    "                textFile.write(text)  # write text to text file\n",
    "\n",
    "    # set paths accordingly:\n",
    "    pdfDir = \"./\"\n",
    "    txtDir = \"./\"\n",
    "    convertMultiple(pdfDir, txtDir)\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_nlp(input_paragraph, looking_for=None):\n",
    "        \n",
    "    # First, handle pronoun replacement\n",
    "    replacements = {'he': ' [THEY]',\n",
    "                      'him': ' [THEM]',\n",
    "                      'his': ' [THEIR]',\n",
    "                      'she': ' [THEY]',\n",
    "                      'her': ' [THEM]',\n",
    "                      'hers': ' [THEIR]',\n",
    "                      'man': '[PERSON]',\n",
    "                      'woman': '[PERSON]',\n",
    "                      'men': '[PEOPLE]',\n",
    "                      'women': '[PEOPLE]'\n",
    "                     }\n",
    "\n",
    "    def replace_pronouns(match):\n",
    "        return replacements[match.group(0)]\n",
    "\n",
    "    regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in replacements)\n",
    "    input_paragraph = re.sub(regex, replace_pronouns, input_paragraph)\n",
    "    \n",
    "    \n",
    "    # Redact Emails\n",
    "    regex_email = '[a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+'\n",
    "    lst_emails = re.findall(regex_email, input_paragraph)\n",
    "    for email in lst_emails:\n",
    "        input_paragraph = input_paragraph.replace(email,\"[EMAIL]\")\n",
    "\n",
    "    # Redact Phone numbers\n",
    "    regex_phone = '\\d{3}[-\\.\\s]+\\d{3}[-\\.\\s]+\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]+\\d{4}'\n",
    "    lst_phones = re.findall(regex_phone, input_paragraph)\n",
    "    for phone in lst_phones:\n",
    "        input_paragraph = input_paragraph.replace(phone,\"[PHONE]\")\n",
    "\n",
    "    # Redact Dates\n",
    "    #regex_date = \"((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+\\d{1,2},\\s+\\d{4})\"\n",
    "    #regex_date = '[\\d]{1,2}[/-][\\d]{1,2}[/-][\\d]{4}'\n",
    "    regex_date = \"([\\d]{1,2}[/-][\\d]{1,2}[/-][\\d]{4})|((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+\\d{1,2},\\s+\\d{4})|((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s?[\\d{1,2}]?[,]?\\s+\\d{4})|(\\d{4}[-]+\\d{4})\"\n",
    "    lst_dates = re.findall(regex_date, input_paragraph)\n",
    "    for date in lst_dates:\n",
    "        date = max(date, key = len)\n",
    "        input_paragraph = input_paragraph.replace(date,\"[DATE]\")\n",
    "    \n",
    "    # Redact Address/Location, Company/Organization, and Names using stanza\n",
    "    try:\n",
    "        doc = nlp(input_paragraph)\n",
    "        lst_entities = {\n",
    "            'FAC': '[ADDRESS]',\n",
    "            'GPE': '[LOCATION]',\n",
    "            'PERSON': '[PERSON]',\n",
    "            'ORG': '[ORGANIZATION]',\n",
    "            'LOC': '[LOCATION]'\n",
    "        }\n",
    "        for sentence in doc.sentences:\n",
    "            for i in sentence.ents:\n",
    "                if i.type in lst_entities and i.text not in ['PERSON', 'EMAIL']:\n",
    "                    input_paragraph = input_paragraph.replace(i.text,lst_entities[i.type])\n",
    "    except:\n",
    "        print(\"corenlp didn't work..\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PHONE] [PHONE] [PHONE] [PHONE] [PHONE]\n"
     ]
    }
   ],
   "source": [
    "# Testing Phone Numbers:\n",
    "input_paragraph = \"303-888-6666 (303) 888 6666 (303) 888-6666 303 888 6666 303.888.6666\"\n",
    "core_nlp(input_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL] [EMAIL]\n"
     ]
    }
   ],
   "source": [
    "# Testing Emails:\n",
    "input_paragraph = \"\"\"bryce.schumacher8@gmail.com brsc9114@colorado.edu bryce.schumacher@colorado.edu thoughtlessEduardo14@charter.net Meaganfaithful@yahoo.in comfortableCassandra46@bol.com.br cloudyLisa88@yahoo.fr Tristanperfect@ymail.com enchantingKelly@arcor.de grumpyOlivia0@ymail.com Audreysleepy@live.ca goodJulie@hotmail.com anxiousKrystal@gmx.de grotesqueAlisha54@googlemail.com arrogantGerald@yahoo.com.br cleanAndres@live.it mistyTracy@home.nl annoyingDominique7@yahoo.es energeticAndrew@hetnet.nl boredGrant11@bluewin.ch unsightlyJason45@live.com.au oddLinda33@wanadoo.fr gorgeousPatricia70@wanadoo.fr gentleMelinda@mail.ru grotesqueRenee@hotmail.com Prestonprecious@t-online.de enchantingNatalie79@bellsouth.net shyDevin@zonnet.nl obedientKathleen@yahoo.es cleverLarry63@yahoo.com cruelRebecca62@hotmail.co.uk betterCody3@skynet.be zanyAna@hotmail.co.uk puzzledDavid@yahoo.com.ar Geoffreyfunny@yahoo.co.id zanyDenise19@arcor.de unusualMitchell21@cox.net drabThomas80@bigpond.net.au Meghanugliest@zonnet.nl comfortableGrace56@voila.fr upsetTristan@aol.com Mitchellpleasant@libero.it jitteryArthur24@live.com.au vivaciousBrittany@club-internet.fr Martinblack@live.co.uk jealousDominic@freenet.de easyShaun@mail.com toughSummer@facebook.com Meaganhomely@yahoo.com.au Marvinhappy@planet.nl longLeslie@verizon.net expensiveAdam@t-online.de Marctense@neuf.fr smoggyDevin57@live.ca Marcuscruel@verizon.net Catherinezany@bol.com.br lazyJacqueline85@sympatico.ca difficultAaron11@centurytel.net cuteMichael@t-online.de Nathanielclever@live.co.uk dizzyLee87@juno.com defeatedVirginia35@hotmail.es franticRonnie@yahoo.co.uk scaryAlan@yahoo.com.mx Kristinexcited@ntlworld.com dizzyBobby@hotmail.es inquisitiveAudrey61@frontiernet.net Ianlucky@sky.com Grantfrail@yahoo.co.in Jonathonhelpless@gmx.de ashamedTrevor92@facebook.com cuteAshlee@optusnet.com.au Kaitlinunsightly@yahoo.ca vivaciousJordan42@centurytel.net upsetMaria@gmx.de badLance@centurytel.net Jimmyfantastic@yahoo.co.id successfulCatherine74@virgilio.it cleverLisa@sympatico.ca Kristopherproud@blueyonder.co.uk magnificentGary@live.nl upsetChristie25@yahoo.fr blue-eyedVincent69@bluewin.ch angryJared@voila.fr Markhilarious@tiscali.co.uk repulsiveKathleen@neuf.fr nastyCandace40@t-online.de easyAaron@hotmail.it bluePeter48@verizon.net depressedLori78@yahoo.it Jerryblue-eyed@optonline.net toughHenry17@ntlworld.com disgustedLuis@sympatico.ca scaryErin@hetnet.nl Michealexuberant@planet.nl Virginiamushy@tin.it jealousCarolyn68@yahoo.co.in bewilderedJill53@laposte.net Troymotionless@virgilio.it perfectKristi57@telenet.be Margaretunusual@tiscali.it kindChristie@t-online.de sparklingPriscilla67@att.net\"\"\"\n",
    "core_nlp(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [THEY]  [THEM]  [THEIR]  [THEY]  [THEM]  [THEIR] [PERSON] [PERSON] [PEOPLE] [PEOPLE]\n"
     ]
    }
   ],
   "source": [
    "# Testing pronouns:\n",
    "input_paragraph = \"he him his she her hers man woman men women\"\n",
    "core_nlp(input_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATE] [DATE] [DATE] [DATE] [DATE] [DATE] - [DATE] [DATE]\n"
     ]
    }
   ],
   "source": [
    "# Testing dates:\n",
    "input_paragraph = \"5/10/2022 5-10-2022 5-10-2022 5/10/2022 May 5, 2022 May 2022 - June 2022 1992-2023\"\n",
    "core_nlp(input_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to [ORGANIZATION]. I went to [ORGANIZATION]. I went to [ORGANIZATION]. I went to [ORGANIZATION]. I worked at [ORGANIZATION]. I worked at [ORGANIZATION].\n"
     ]
    }
   ],
   "source": [
    "# Testing Company / Organization Names\n",
    "input_paragraph = \"I went to Mountain Vista High School. I went to Boulder High School. I went to University of Colorado Boulder. I went to Google. I worked at Facebook. I worked at Amazon.\"\n",
    "core_nlp(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_with_pdf_miner()\n",
    "f = open(\"./sample_cover_letter.pdf.txt\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Cover Letter\n",
      "\n",
      "October 10, 2021\n",
      "\n",
      "Susan Carey \n",
      "Senior Manager \n",
      "Wholesale Wine USA \n",
      "23 Green St. \n",
      "Boston, MA  02116 \n",
      "\n",
      "Dear Susan Carey: \n",
      "\n",
      "I am writing to apply for your position in wine wholesale as advertised on Crimson Careers. This \n",
      "exciting opportunity appears to be a wonderful fit with my professional experience, personal \n",
      "interests, and career goals. \n",
      "\n",
      "I am returning to Boston to complete my final year at Harvard University Extension School, \n",
      "where I am majoring in French and economics. Having spent the year working and traveling, I am \n",
      "eager to incorporate myself once again into the local wine community, to which I can bring \n",
      "experience in a number of sectors of the industry.  \n",
      "\n",
      "Through eight years in the restaurant field, I have acquired a deep love of and appreciation for \n",
      "wine and cuisine.  I have been known to wax rhapsodic over specials; nothing made me happier \n",
      "than discussing a bottle with a table. This enthusiasm allowed me to introduce a list of reserve \n",
      "selections to Shayâ€™s Pub and Wine Bar. The result was an appreciable increase in sales for the \n",
      "restaurant and repeat attendance by customers. My position at Aspen's award-winning Montagna \n",
      "allowed me to expand upon my knowledge of wine, locally inspired cuisine, and the highest \n",
      "standards of service. Our weekly blind-tastings fueled my desire to further myself in this field, \n",
      "and I am in the process of acquiring certification through both the Court of Master Sommeliers \n",
      "and the Wine Spirit and Education Trust. \n",
      "\n",
      "Most recently, I have returned from France where I was lucky enough to work on an organic \n",
      "vineyard in Beaujolais. I adored working with the young, dynamic, vigneron who ran the estate, \n",
      "the largest of its kind in the region. A position at your wholesale wine company would allow me \n",
      "to draw upon this experience and to facilitate the success of such producers. Additionally, it \n",
      "would enable me to replicate the most enjoyable components of my experience overall: working \n",
      "with my colleagues in the local restaurant industry, as well as with distinctive, iconoclastic wine-\n",
      "makers.\n",
      "\n",
      "I am readily available via email or phone in order to arrange an interview, and have attached my \n",
      "resume below per your request. Please do not hesitate to contact me if you have any questions. I \n",
      "appreciate your consideration and look forward to hearing from you. \n",
      "\n",
      "Sincerely, \n",
      "\n",
      "Georgina Santiago \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Cover Letter\n",
      "\n",
      "[DATE]\n",
      "\n",
      "[PERSON] \n",
      "Senior Manager \n",
      "Wholesale Wine [LOCATION] \n",
      "23 Green St. \n",
      "[LOCATION], [LOCATION]  02116 \n",
      "\n",
      "Dear [PERSON]: \n",
      "\n",
      "I am writing to apply for your position in wine wholesale as advertised on [ORGANIZATION]. This \n",
      "exciting opportunity appears to be a wonderful fit with my professional experience, personal \n",
      "interests, and career goals. \n",
      "\n",
      "I am returning to [LOCATION] to complete my final year at [ORGANIZATION], \n",
      "where I am majoring in French and economics. Having spent the year working and traveling, I am \n",
      "eager to incorporate myself once again into the local wine community, to which I can bring \n",
      "experience in a number of sectors of the industry.  \n",
      "\n",
      "Through eight years in the restaurant field, I have acquired a deep love of and appreciation for \n",
      "wine and cuisine.  I have been known to wax rhapsodic over specials; nothing made me happier \n",
      "than discussing a bottle with a table. This enthusiasm allowed me to introduce a list of reserve \n",
      "selections to [ORGANIZATION]. The result was an appreciable increase in sales for the \n",
      "restaurant and repeat attendance by customers. My position at [ORGANIZATION]'s award-winning [LOCATION] \n",
      "allowed me to expand upon my knowledge of wine, locally inspired cuisine, and the highest \n",
      "standards of service. Our weekly blind-tastings fueled my desire to further myself in this field, \n",
      "and I am in the process of acquiring certification through both [ORGANIZATION] \n",
      "and [ORGANIZATION]. \n",
      "\n",
      "Most recently, I have returned from [LOCATION] where I was lucky enough to work on an organic \n",
      "vineyard in [LOCATION]. I adored working with the young, dynamic, vigneron who ran the estate, \n",
      "the largest of its kind in the region. A position at your wholesale wine company would allow me \n",
      "to draw upon this experience and to facilitate the success of such producers. Additionally, it \n",
      "would enable me to replicate the most enjoyable components of my experience overall: working \n",
      "with my colleagues in the local restaurant industry, as well as with distinctive, iconoclastic wine-\n",
      "makers.\n",
      "\n",
      "I am readily available via email or phone in order to arrange an interview, and have attached my \n",
      "resume below per your request. Please do not hesitate to contact me if you have any questions. I \n",
      "appreciate your consideration and look forward to hearing from you. \n",
      "\n",
      "Sincerely, \n",
      "\n",
      "[PERSON] \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on a resume\n",
    "core_nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 14:01:55 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-04-12 14:01:55 INFO: Use device: cpu\n",
      "2022-04-12 14:01:55 INFO: Loading: tokenize\n",
      "2022-04-12 14:01:55 INFO: Loading: pos\n",
      "2022-04-12 14:01:55 INFO: Loading: lemma\n",
      "2022-04-12 14:01:55 INFO: Loading: depparse\n",
      "2022-04-12 14:01:55 INFO: Loading: sentiment\n",
      "2022-04-12 14:01:56 INFO: Loading: constituency\n",
      "2022-04-12 14:01:56 INFO: Loading: ner\n",
      "2022-04-12 14:01:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b\\ he\\b|\\b\\ him\\b|\\b\\ his\\b|\\b\\ she\\b|\\b\\ her\\b|\\b\\ hers\\b\n",
      "It was another early sunset on a rainy day in LOCATION. PERSON was walking with a paper bag of groceries back to THEIR downtown studio apartment at ADDRESS. He had gotten government permission to stay in this apartment as part of the ORGANIZATION program, focusing on bettering drug-affected neighborhoods in LOCATION. It was an artist supplement program, and PERSON was a published haiku poet and teacher of haiku in public education and sometimes colleges. He had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown LOCATION for $800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of THEIR haiku and accompanying minimalist art made by THEIR neighbor, PERSON.\n"
     ]
    }
   ],
   "source": [
    "input_paragraph = \"It was another early sunset on a rainy day in Seattle. Andrew was walking with a paper bag of groceries back to his downtown studio apartment at 400 Main Street. He had gotten government permission to stay in this apartment as part of the Upbeat program, focusing on bettering drug-affected neighborhoods in King County. It was an artist supplement program, and Andrew was a published haiku poet and teacher of haiku in public education and sometimes colleges. He had to attend an audition for artistic achievements to be able to get the subsidy. Though it was a miracle to live in downtown Seattle for $800 a month, it did have its downsides, with junkies visiting the weekly exhibitions of his haiku and accompanying minimalist art made by his neighbor, Patrick.\"\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "doc = nlp(input_paragraph)\n",
    "# Doesn't include date, cardinal, maybe more\n",
    "lst_entities = {\n",
    "    'FAC': 'ADDRESS',\n",
    "    'GPE': 'LOCATION',\n",
    "    'PERSON': 'PERSON',\n",
    "    'ORG': 'ORGANIZATION'\n",
    "}\n",
    "for sentence in doc.sentences:\n",
    "#     print(sentence.ents)\n",
    "    for i in sentence.ents:\n",
    "        if i.type in lst_entities:\n",
    "            input_paragraph = input_paragraph.replace(i.text,lst_entities[i.type])\n",
    "replacements = {' he': ' THEY',\n",
    "                      ' him': ' THEM',\n",
    "                      ' his': ' THEIR',\n",
    "                      ' she': ' THEY',\n",
    "                      ' her': ' THEM',\n",
    "                      ' hers': ' THEIR',\n",
    "                     }\n",
    "\n",
    "def replace(match):\n",
    "    return replacements[match.group(0)]\n",
    "\n",
    "# notice that the 'this' in 'thistle' is not matched \n",
    "regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in replacements)\n",
    "print(regex)\n",
    "new_txt = re.sub(regex, replace, input_paragraph)\n",
    "print(new_txt)\n",
    "\n",
    "# for key, value in lst_manual_changes.items():\n",
    "#     print(key,value)\n",
    "#     input_paragraph = input_paragraph.replace(key,value)\n",
    "# print(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Names\n",
      "0     Abbey\n",
      "1     Abbie\n",
      "2      Abby\n",
      "3     Abdul\n",
      "4       Abe\n",
      "..      ...\n",
      "495  Blaine\n",
      "496   Blair\n",
      "497   Blake\n",
      "498  Blanca\n",
      "499  Blanch\n",
      "\n",
      "[500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "names_info = pd.read_csv('us.txt')\n",
    "names_info.columns = ['Names']\n",
    "names_info = names_info[:500]\n",
    "print(names_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_nlp(input_paragraph, looking_for=None):\n",
    "    doc = nlp(input_paragraph)\n",
    "    # Doesn't include date, cardinal, maybe more\n",
    "    lst_entities = {\n",
    "        'FAC': '[ADDRESS]',\n",
    "        'GPE': '[LOCATION]',\n",
    "        'PERSON': '[PERSON]',\n",
    "        'ORG': '[ORGANIZATION]',\n",
    "        'DATE': '[DATE]',\n",
    "        'LOC': '[LOCATION]'\n",
    "    }\n",
    "    for sentence in doc.sentences:\n",
    "        for i in sentence.ents:\n",
    "            if i.type in lst_entities:\n",
    "                if looking_for:\n",
    "                    if i.type in looking_for:\n",
    "                        return True\n",
    "    return False\n",
    "#                 input_paragraph = input_paragraph.replace(i.text,lst_entities[i.type])\n",
    "    replacements = {' he': ' [THEY]',\n",
    "                          ' him': ' [THEM]',\n",
    "                          ' his': ' [THEIR]',\n",
    "                          ' she': ' [THEY]',\n",
    "                          ' her': ' [THEM]',\n",
    "                          ' hers': ' [THEIR]',\n",
    "                          'man': '[PERSON]',\n",
    "                          'woman': '[PERSON]'\n",
    "                         }\n",
    "\n",
    "    def replace(match):\n",
    "        return replacements[match.group(0)]\n",
    "\n",
    "    # notice that the 'this' in 'thistle' is not matched \n",
    "    regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in replacements)\n",
    "    new_txt = re.sub(regex, replace, input_paragraph)\n",
    "\n",
    "\n",
    "    regex_email = '[a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+'\n",
    "    lst_emails = re.findall(regex_email, new_txt)\n",
    "    for email in lst_emails:\n",
    "        new_txt = new_txt.replace(email,\"[EMAIL]\")\n",
    "\n",
    "    regex_phone = '\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}'\n",
    "    lst_phones = re.findall(regex_phone, new_txt)\n",
    "    for phone in lst_phones:\n",
    "#         print(phone)\n",
    "        new_txt = new_txt.replace(phone,\"[PHONE NUMBER]\")\n",
    "\n",
    "    regex_SSN = '[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}'\n",
    "    lst_SSN = re.findall(regex_SSN, new_txt)\n",
    "    for ssn in lst_SSN:\n",
    "#         print(ssn)\n",
    "        new_txt = new_txt.replace(ssn,\"[DATE]\")\n",
    "#     print(new_txt)\n",
    "\n",
    "\n",
    "    # 12-13-2020\n",
    "    # SSN's\n",
    "    # Citizenship Status\n",
    "\n",
    "    # for key, value in lst_manual_changes.items():\n",
    "    #     print(key,value)\n",
    "    #     input_paragraph = input_paragraph.replace(key,value)\n",
    "    # print(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 17:52:36 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-04-12 17:52:36 INFO: Use device: cpu\n",
      "2022-04-12 17:52:36 INFO: Loading: tokenize\n",
      "2022-04-12 17:52:36 INFO: Loading: pos\n",
      "2022-04-12 17:52:37 INFO: Loading: lemma\n",
      "2022-04-12 17:52:37 INFO: Loading: depparse\n",
      "2022-04-12 17:52:37 INFO: Loading: sentiment\n",
      "2022-04-12 17:52:38 INFO: Loading: constituency\n",
      "2022-04-12 17:52:38 INFO: Loading: ner\n",
      "2022-04-12 17:52:39 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 0.912\n"
     ]
    }
   ],
   "source": [
    "# Name data script:\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "cntr = 0\n",
    "list_input = names_info['Names'][:1000]\n",
    "lst_len = len(list_input)\n",
    "for i in list_input:\n",
    "    ret = run_nlp(i, ['PERSON'])\n",
    "    if ret:\n",
    "        cntr += 1\n",
    "print(f\"Percentage: {cntr/lst_len}\")\n",
    "# nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1745 T Street Southeast\n",
      "[]\n",
      "6007 Applegate Lane\n",
      "[{\n",
      "  \"text\": \"Applegate Lane\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 19\n",
      "}]\n",
      "560 Penstock Drive\n",
      "[{\n",
      "  \"text\": \"560\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "150 Carter Street\n",
      "[{\n",
      "  \"text\": \"150\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "2721 Lindsay Avenue\n",
      "[{\n",
      "  \"text\": \"Lindsay Avenue\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 19\n",
      "}]\n",
      "18 Densmore Drive\n",
      "[{\n",
      "  \"text\": \"18\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}]\n",
      "637 Britannia Drive\n",
      "[{\n",
      "  \"text\": \"637 Britannia Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 19\n",
      "}]\n",
      "5601 West Crocus Drive\n",
      "[{\n",
      "  \"text\": \"5601 West Crocus Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 22\n",
      "}]\n",
      "5403 Illinois Avenue\n",
      "[{\n",
      "  \"text\": \"Illinois Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 20\n",
      "}]\n",
      "8821 West Myrtle Avenue\n",
      "[{\n",
      "  \"text\": \"8821 West Myrtle Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 23\n",
      "}]\n",
      "2203 7th Street Road\n",
      "[{\n",
      "  \"text\": \"7th\",\n",
      "  \"type\": \"ORDINAL\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 8\n",
      "}]\n",
      "6463 Vrain Street\n",
      "[]\n",
      "87 Horseshoe Drive\n",
      "[{\n",
      "  \"text\": \"87 Horseshoe Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 18\n",
      "}]\n",
      "60 Desousa Drive\n",
      "[{\n",
      "  \"text\": \"60\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}, {\n",
      "  \"text\": \"Desousa Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 3,\n",
      "  \"end_char\": 16\n",
      "}]\n",
      "4 Old Colony Way\n",
      "[{\n",
      "  \"text\": \"4\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 1\n",
      "}]\n",
      "314 South 17th Street\n",
      "[]\n",
      "1649 Timberridge Court\n",
      "[]\n",
      "5461 West Shades Valley Drive\n",
      "[{\n",
      "  \"text\": \"West Shades Valley Drive\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 29\n",
      "}]\n",
      "629 Debbie Drive\n",
      "[{\n",
      "  \"text\": \"629\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "22572 Toreador Drive\n",
      "[]\n",
      "3034 Mica Street\n",
      "[]\n",
      "3729 East Mission Boulevard\n",
      "[{\n",
      "  \"text\": \"3729 East Mission Boulevard\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 27\n",
      "}]\n",
      "5114 Greentree Drive\n",
      "[{\n",
      "  \"text\": \"Greentree Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 20\n",
      "}]\n",
      "3466 Southview Avenue\n",
      "[{\n",
      "  \"text\": \"Southview Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 21\n",
      "}]\n",
      "1513 Cathy Street\n",
      "[{\n",
      "  \"text\": \"Cathy Street\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 17\n",
      "}]\n",
      "600 West 19th Avenue\n",
      "[{\n",
      "  \"text\": \"600\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "1208 Elkader Court North\n",
      "[]\n",
      "210 Green Road\n",
      "[{\n",
      "  \"text\": \"210\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "49548 Road 200\n",
      "[{\n",
      "  \"text\": \"200\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 11,\n",
      "  \"end_char\": 14\n",
      "}]\n",
      "81 Seaton Place Northwest\n",
      "[{\n",
      "  \"text\": \"81\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}, {\n",
      "  \"text\": \"Seaton Place Northwest\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 3,\n",
      "  \"end_char\": 25\n",
      "}]\n",
      "1267 Martin Street\n",
      "[{\n",
      "  \"text\": \"Martin Street\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 18\n",
      "}]\n",
      "7431 Candace Way\n",
      "[{\n",
      "  \"text\": \"Candace Way\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 16\n",
      "}]\n",
      "1407 Walden Court\n",
      "[{\n",
      "  \"text\": \"Walden Court\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 17\n",
      "}]\n",
      "5906 Milton Avenue\n",
      "[{\n",
      "  \"text\": \"Milton Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 18\n",
      "}]\n",
      "74 Springfield Street\n",
      "[{\n",
      "  \"text\": \"74\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}]\n",
      "2905 Stonebridge Court\n",
      "[]\n",
      "20930 Todd Valley Road\n",
      "[{\n",
      "  \"text\": \"Todd Valley Road\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 6,\n",
      "  \"end_char\": 22\n",
      "}]\n",
      "5928 West Mauna Loa Lane\n",
      "[]\n",
      "802 Madison Street Northwest\n",
      "[]\n",
      "2811 Battery Place Northwest\n",
      "[]\n",
      "210 Lacross Lane\n",
      "[{\n",
      "  \"text\": \"210\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "2010 Rising Hill Drive\n",
      "[{\n",
      "  \"text\": \"2010\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 4\n",
      "}]\n",
      "388 East Main Street\n",
      "[]\n",
      "450 Kinhawk Drive\n",
      "[{\n",
      "  \"text\": \"450\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "131 Westerly Street\n",
      "[{\n",
      "  \"text\": \"131\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "308 Woodleaf Court\n",
      "[{\n",
      "  \"text\": \"308\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "8502 Madrone Avenue\n",
      "[{\n",
      "  \"text\": \"Madrone Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 19\n",
      "}]\n",
      "23 Sable Run Lane\n",
      "[{\n",
      "  \"text\": \"23\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}]\n",
      "716 Waller Road\n",
      "[{\n",
      "  \"text\": \"716\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "416 McIver Street\n",
      "[]\n",
      "1508 Massachusetts Avenue Southeast\n",
      "[{\n",
      "  \"text\": \"Massachusetts\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 18\n",
      "}]\n",
      "5615 West Villa Maria Drive\n",
      "[{\n",
      "  \"text\": \"West Villa\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 15\n",
      "}, {\n",
      "  \"text\": \"Maria Drive\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 16,\n",
      "  \"end_char\": 27\n",
      "}]\n",
      "3162 Martin Luther King Junior Boulevard\n",
      "[{\n",
      "  \"text\": \"Martin Luther King Junior Boulevard\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 40\n",
      "}]\n",
      "5306 Ritchie Highway\n",
      "[{\n",
      "  \"text\": \"Ritchie Highway\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 20\n",
      "}]\n",
      "109 Summit Street\n",
      "[]\n",
      "816 West 19th Avenue\n",
      "[]\n",
      "172 Alburg Springs Road\n",
      "[{\n",
      "  \"text\": \"172\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "159 Downey Drive\n",
      "[{\n",
      "  \"text\": \"159\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "125 John Street\n",
      "[{\n",
      "  \"text\": \"John Street\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 4,\n",
      "  \"end_char\": 15\n",
      "}]\n",
      "1101 Lotus Avenue\n",
      "[]\n",
      "8376 Albacore Drive\n",
      "[{\n",
      "  \"text\": \"8376 Albacore Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 19\n",
      "}]\n",
      "491 Arabian Way\n",
      "[{\n",
      "  \"text\": \"491\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "12245 West 71st Place\n",
      "[]\n",
      "80 North East Street\n",
      "[{\n",
      "  \"text\": \"80\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}, {\n",
      "  \"text\": \"North East Street\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 3,\n",
      "  \"end_char\": 20\n",
      "}]\n",
      "4695 East Huntsville Road\n",
      "[]\n",
      "310 Timrod Road\n",
      "[{\n",
      "  \"text\": \"310\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "1364 Capri Drive\n",
      "[{\n",
      "  \"text\": \"Capri Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 16\n",
      "}]\n",
      "132 Laurel Green Court\n",
      "[{\n",
      "  \"text\": \"132\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "6657 West Rose Garden Lane\n",
      "[]\n",
      "519 West 75th Avenue\n",
      "[]\n",
      "31353 Santa Elena Way\n",
      "[]\n",
      "8398 West Denton Lane\n",
      "[{\n",
      "  \"text\": \"West Denton Lane\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 21\n",
      "}]\n",
      "700 Winston Place\n",
      "[{\n",
      "  \"text\": \"700\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "232 Maine Avenue\n",
      "[{\n",
      "  \"text\": \"232 Maine Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 16\n",
      "}]\n",
      "1 Kempf Drive\n",
      "[{\n",
      "  \"text\": \"1\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 1\n",
      "}, {\n",
      "  \"text\": \"Kempf Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 2,\n",
      "  \"end_char\": 13\n",
      "}]\n",
      "5811 Crossings Boulevard\n",
      "[{\n",
      "  \"text\": \"5811 Crossings Boulevard\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 24\n",
      "}]\n",
      "5108 Franklin Street\n",
      "[{\n",
      "  \"text\": \"Franklin Street\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 20\n",
      "}]\n",
      "913 Fallview Trail\n",
      "[{\n",
      "  \"text\": \"913\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "270 Chrissy's Court\n",
      "[{\n",
      "  \"text\": \"270\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}]\n",
      "130 Old Route 103\n",
      "[{\n",
      "  \"text\": \"130\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "}, {\n",
      "  \"text\": \"Route 103\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 8,\n",
      "  \"end_char\": 17\n",
      "}]\n",
      "10826 Pointe Royal Drive\n",
      "[{\n",
      "  \"text\": \"10826 Pointe Royal Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 24\n",
      "}]\n",
      "74 Ranch Drive\n",
      "[{\n",
      "  \"text\": \"74\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}, {\n",
      "  \"text\": \"Ranch Drive\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 3,\n",
      "  \"end_char\": 14\n",
      "}]\n",
      "6601 West Ocotillo Road\n",
      "[]\n",
      "19416 Barclay Road\n",
      "[{\n",
      "  \"text\": \"Barclay Road\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 6,\n",
      "  \"end_char\": 18\n",
      "}]\n",
      "1347 Blackwalnut Court\n",
      "[]\n",
      "1770 Colony Way\n",
      "[]\n",
      "165 Saint John Street\n",
      "[]\n",
      "2409 Research Boulevard\n",
      "[]\n",
      "1903 Bashford Manor Lane\n",
      "[{\n",
      "  \"text\": \"1903\",\n",
      "  \"type\": \"DATE\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 4\n",
      "}, {\n",
      "  \"text\": \"Bashford Manor Lane\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 24\n",
      "}]\n",
      "8315 Surf Drive\n",
      "[]\n",
      "3301 Old Muldoon Road\n",
      "[]\n",
      "8800 Cordell Circle\n",
      "[]\n",
      "117 East Cook Avenue\n",
      "[{\n",
      "  \"text\": \"117 East Cook Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 20\n",
      "}]\n",
      "6231 North 67th Avenue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"6231 North 67th Avenue\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 22\n",
      "}]\n",
      "8505 Waters Avenue\n",
      "[]\n",
      "7 Underwood Place Northwest\n",
      "[{\n",
      "  \"text\": \"7\",\n",
      "  \"type\": \"CARDINAL\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 1\n",
      "}]\n",
      "21950 Arnold Center Road\n",
      "[{\n",
      "  \"text\": \"Arnold Center Road\",\n",
      "  \"type\": \"FAC\",\n",
      "  \"start_char\": 6,\n",
      "  \"end_char\": 24\n",
      "}]\n",
      "1427 South Carolina Avenue Southeast\n",
      "[{\n",
      "  \"text\": \"South Carolina\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 19\n",
      "}, {\n",
      "  \"text\": \"Avenue Southeast\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 20,\n",
      "  \"end_char\": 36\n",
      "}]\n",
      "1420 Turtleback Trail\n",
      "[]\n",
      "29 99\n",
      "Percentage: 0.29292929292929293\n"
     ]
    }
   ],
   "source": [
    "# Script for Addresses\n",
    "with open(\"addresses-us-100.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "addresses = data['addresses']\n",
    "lst_len = len(addresses)\n",
    "cntr = 0\n",
    "for address in addresses:\n",
    "    print(address['address1'])\n",
    "    \n",
    "    ret = run_nlp(address['address1'], ['FAC','LOC','GPE'])\n",
    "    if ret:\n",
    "        cntr += 1\n",
    "print(cntr,lst_len)\n",
    "print(f\"Percentage: {cntr/lst_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 14:02:19 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-04-12 14:02:19 INFO: Use device: cpu\n",
      "2022-04-12 14:02:19 INFO: Loading: tokenize\n",
      "2022-04-12 14:02:19 INFO: Loading: pos\n",
      "2022-04-12 14:02:20 INFO: Loading: lemma\n",
      "2022-04-12 14:02:20 INFO: Loading: depparse\n",
      "2022-04-12 14:02:21 INFO: Loading: sentiment\n",
      "2022-04-12 14:02:21 INFO: Loading: constituency\n",
      "2022-04-12 14:02:22 INFO: Loading: ner\n",
      "2022-04-12 14:02:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303)888 8888\n",
      "303-884-6663\n",
      "11/15/2020\n",
      "11/13/2020\n",
      "[PHONE NUMBER] [PHONE NUMBER] [ORGANIZATION] [DATE] [EMAIL] [DATE] [ORGANIZATION] [DATE] [PERSON] [PERSON] His nose could have been mistaken for a carrot. Not one of those dirt-rich oblong carrots, but a baby-sized carrot packed in those free-moving, punctured plastic bags. Yet more distinguishable were [THEIR] cheeks, slit in as if a wild animal unintentionally scathed a straight line down the skin by [THEIR] cheekbone. A rugged [PERSON] if I ever saw one. But [THEIR] name wasnâ€™t that beefy: [PERSON]. [PERSON] was a name white collar folk call in for [DATE] dinner for tidbits of entertainment or a chiseled smile. [PERSON] seemed like a more convincing name for [THEM], especially since that nose of [THEIR] could have poked [PERSON] back to eating crÃªpesâ€”but they would have it that this gritty-faced [PERSON] (we havenâ€™t gotten into [THEIR] personality yet) was named [PERSON] at birth. No, itâ€™s not what you thinkâ€”it isnâ€™t [PERSON]. Parents do the darndest things.\n"
     ]
    }
   ],
   "source": [
    "input_paragraph = \"(303)888 8888 303-884-6663 Boulder High School May 26th bryce.schumacher8@gmail.com 11/15/2020 University of Colorado 11/13/2020 man woman His nose could have been mistaken for a carrot. Not one of those dirt-rich oblong carrots, but a baby-sized carrot packed in those free-moving, punctured plastic bags. Yet more distinguishable were his cheeks, slit in as if a wild animal unintentionally scathed a straight line down the skin by his cheekbone. A rugged Pinocchio if I ever saw one. But his name wasnâ€™t that beefy: Marty. Marty was a name white collar folk call in for Sunday dinner for tidbits of entertainment or a chiseled smile. Jared seemed like a more convincing name for him, especially since that nose of his could have poked Napoleon back to eating crÃªpesâ€”but they would have it that this gritty-faced man (we havenâ€™t gotten into his personality yet) was named Marty at birth. No, itâ€™s not what you thinkâ€”it isnâ€™t Martin. Parents do the darndest things.\"\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "doc = nlp(input_paragraph)\n",
    "# Doesn't include date, cardinal, maybe more\n",
    "lst_entities = {\n",
    "    'FAC': '[ADDRESS]',\n",
    "    'GPE': '[LOCATION]',\n",
    "    'PERSON': '[PERSON]',\n",
    "    'ORG': '[ORGANIZATION]',\n",
    "    'DATE': '[DATE]'\n",
    "}\n",
    "for sentence in doc.sentences:\n",
    "#     print(sentence.ents)\n",
    "    for i in sentence.ents:\n",
    "        if i.type in lst_entities:\n",
    "            input_paragraph = input_paragraph.replace(i.text,lst_entities[i.type])\n",
    "replacements = {' he': ' [THEY]',\n",
    "                      ' him': ' [THEM]',\n",
    "                      ' his': ' [THEIR]',\n",
    "                      ' she': ' [THEY]',\n",
    "                      ' her': ' [THEM]',\n",
    "                      ' hers': ' [THEIR]',\n",
    "                      'man': '[PERSON]',\n",
    "                      'woman': '[PERSON]'\n",
    "                     }\n",
    "\n",
    "def replace(match):\n",
    "    return replacements[match.group(0)]\n",
    "\n",
    "# notice that the 'this' in 'thistle' is not matched \n",
    "regex = '|'.join(r'\\b%s\\b' % re.escape(s) for s in replacements)\n",
    "new_txt = re.sub(regex, replace, input_paragraph)\n",
    "\n",
    "\n",
    "regex_email = '[a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+'\n",
    "lst_emails = re.findall(regex_email, new_txt)\n",
    "for email in lst_emails:\n",
    "    new_txt = new_txt.replace(email,\"[EMAIL]\")\n",
    "    \n",
    "regex_phone = '\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}'\n",
    "lst_phones = re.findall(regex_phone, new_txt)\n",
    "for phone in lst_phones:\n",
    "    print(phone)\n",
    "    new_txt = new_txt.replace(phone,\"[PHONE NUMBER]\")\n",
    "\n",
    "regex_SSN = '[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}'\n",
    "lst_SSN = re.findall(regex_SSN, new_txt)\n",
    "for ssn in lst_SSN:\n",
    "    print(ssn)\n",
    "    new_txt = new_txt.replace(ssn,\"[DATE]\")\n",
    "print(new_txt)\n",
    "\n",
    "\n",
    "# 12-13-2020\n",
    "# SSN's\n",
    "# Citizenship Status\n",
    "\n",
    "# for key, value in lst_manual_changes.items():\n",
    "#     print(key,value)\n",
    "#     input_paragraph = input_paragraph.replace(key,value)\n",
    "# print(input_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
